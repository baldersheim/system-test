{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import zstandard as zstd  # You need the zstandard library for compression.\n",
    "\n",
    "# Constants\n",
    "OUTPUT_DIR = \"./output-data/\"\n",
    "FINAL_DIR = OUTPUT_DIR + \"final/\"\n",
    "\n",
    "# Vespa constants\n",
    "VESPA_DOCTYPE = \"product\"\n",
    "VESPA_NAMESPACE = \"product\"\n",
    "\n",
    "# ES constants\n",
    "ES_INDEX = \"product\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('output-data/CDs_and_Vinyl_processed.parquet'),\n",
       " PosixPath('output-data/Baby_Products_processed.parquet'),\n",
       " PosixPath('output-data/Movies_and_TV_processed.parquet'),\n",
       " PosixPath('output-data/Office_Products_processed.parquet'),\n",
       " PosixPath('output-data/Electronics_processed.parquet'),\n",
       " PosixPath('output-data/Toys_and_Games_processed.parquet'),\n",
       " PosixPath('output-data/Health_and_Household_processed.parquet'),\n",
       " PosixPath('output-data/All_Beauty_processed.parquet'),\n",
       " PosixPath('output-data/Pet_Supplies_processed.parquet'),\n",
       " PosixPath('output-data/Cell_Phones_and_Accessories_processed.parquet'),\n",
       " PosixPath('output-data/Subscription_Boxes_processed.parquet'),\n",
       " PosixPath('output-data/Grocery_and_Gourmet_Food_processed.parquet'),\n",
       " PosixPath('output-data/Video_Games_processed.parquet'),\n",
       " PosixPath('output-data/Arts_Crafts_and_Sewing_processed.parquet'),\n",
       " PosixPath('output-data/Amazon_Fashion_processed.parquet'),\n",
       " PosixPath('output-data/Sports_and_Outdoors_processed.parquet'),\n",
       " PosixPath('output-data/Beauty_and_Personal_Care_processed.parquet'),\n",
       " PosixPath('output-data/Magazine_Subscriptions_processed.parquet'),\n",
       " PosixPath('output-data/Books_processed.parquet'),\n",
       " PosixPath('output-data/Tools_and_Home_Improvement_processed.parquet'),\n",
       " PosixPath('output-data/Clothing_Shoes_and_Jewelry_processed.parquet'),\n",
       " PosixPath('output-data/Industrial_and_Scientific_processed.parquet'),\n",
       " PosixPath('output-data/Software_processed.parquet'),\n",
       " PosixPath('output-data/Digital_Music_processed.parquet'),\n",
       " PosixPath('output-data/Health_and_Personal_Care_processed.parquet'),\n",
       " PosixPath('output-data/Patio_Lawn_and_Garden_processed.parquet'),\n",
       " PosixPath('output-data/Musical_Instruments_processed.parquet'),\n",
       " PosixPath('output-data/Handmade_Products_processed.parquet'),\n",
       " PosixPath('output-data/Home_and_Kitchen_processed.parquet'),\n",
       " PosixPath('output-data/Gift_Cards_processed.parquet'),\n",
       " PosixPath('output-data/Automotive_processed.parquet'),\n",
       " PosixPath('output-data/Unknown_processed.parquet'),\n",
       " PosixPath('output-data/Kindle_Store_processed.parquet'),\n",
       " PosixPath('output-data/Appliances_processed.parquet')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all parquet files in the output directory\n",
    "parquet_files = list(Path(OUTPUT_DIR).rglob(\"*.parquet\"))\n",
    "parquet_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and join all parquet files\n",
    "df = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382791"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381739"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates on title and description.\n",
    "df = df.drop_duplicates(subset=[\"title\", \"description\"])\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset id-column to integer index\n",
    "df[\"id\"] = df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 1M first rows\n",
    "# df = df.iloc[:1_000_000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381739"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any of the descriptions are empty strings (stripped for whitespace)\n",
    "assert df[\"description\"].str.strip().eq(\"\").sum() == 0\n",
    "# Check that no ids are duplicated\n",
    "assert df[\"id\"].duplicated().sum() == 0\n",
    "# Check that no NaN values are present\n",
    "assert df.isnull().sum().values.sum() == 0\n",
    "# Check for no duplicates on title and description\n",
    "assert df[[\"title\", \"description\"]].duplicated().sum() == 0\n",
    "# Assert length is 1M\n",
    "# assert len(df) == 1_000_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_vespa_format(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: Path,\n",
    "    compression: str = \"zstd\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Transform the DataFrame to a Vespa-compatible format.\n",
    "    Save transformed data to a newline-separated JSON file.\n",
    "    \"\"\"\n",
    "    df[\"docid\"] = (\n",
    "        f\"id:{VESPA_DOCTYPE}:{VESPA_NAMESPACE}::\"\n",
    "        + df[\"category\"]\n",
    "        + df[\"id\"].astype(str)\n",
    "    )\n",
    "    df = df.apply(\n",
    "        lambda row: {\n",
    "            \"put\": row[\"docid\"],\n",
    "            \"fields\": {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"category\": row[\"category\"],\n",
    "                \"description\": row[\"description\"],\n",
    "                \"price\": row[\"price\"],\n",
    "                \"average_rating\": row[\"average_rating\"],\n",
    "                \"embedding\": row[\"embedding\"].tolist(),\n",
    "            },\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    if compression == \"zstd\":\n",
    "        file_name = file_name.with_suffix(\".json.zst\")\n",
    "    df.to_json(file_name, orient=\"records\", lines=True, compression=compression)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_df_to_es_format(\n",
    "    df: pd.DataFrame,\n",
    "    file_name: str,\n",
    "    compression: str = \"zstd\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Transform the DataFrame to an Elasticsearch-compatible format.\n",
    "    Save transformed data to a newline-separated JSON file.\n",
    "    \"\"\"\n",
    "    if compression == \"zstd\":\n",
    "        file_name = file_name.with_suffix(\".json.zst\")\n",
    "    data = \"\"\n",
    "    for index, row in df.iterrows():\n",
    "        action = {\"index\": {\"_index\": ES_INDEX, \"_id\": str(row[\"id\"])}}\n",
    "        data += json.dumps(action, ensure_ascii=True) + \"\\n\"\n",
    "        doc_data = {\n",
    "            \"title\": row[\"title\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"description\": row[\"description\"],\n",
    "            \"price\": row[\"price\"],\n",
    "            \"average_rating\": row[\"average_rating\"],\n",
    "            \"embedding\": row[\"embedding\"].tolist(),\n",
    "        }\n",
    "        data += json.dumps(doc_data, ensure_ascii=True) + \"\\n\"\n",
    "    cctx = zstd.ZstdCompressor()\n",
    "    compressed_data = cctx.compress(data.encode(\"utf-8\"))\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        f.write(compressed_data)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m vespa_save_path \u001b[38;5;241m=\u001b[39m Path(OUTPUT_DIR) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvespa_sample-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m es_save_path \u001b[38;5;241m=\u001b[39m Path(OUTPUT_DIR) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mes_sample-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m save_df_to_vespa_format(df, vespa_save_path)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# save_df_to_es_format(df, es_save_path)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/envs/dp/lib/python3.11/site-packages/pandas/core/generic.py:6118\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6116\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[0;32m-> 6118\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6119\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   6121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/envs/dp/lib/python3.11/site-packages/pandas/core/sample.py:152\u001b[0m, in \u001b[0;36msample\u001b[0;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    153\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1001\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# Write a sample to both vespa and es\n",
    "num_samples = 100_000\n",
    "vespa_save_path = Path(OUTPUT_DIR) / f\"vespa_sample-{num_samples}.jsonl\"\n",
    "es_save_path = Path(OUTPUT_DIR) / f\"es_sample-{num_samples}.jsonl\"\n",
    "df = df.sample(num_samples, random_state=42)\n",
    "save_df_to_vespa_format(df, vespa_save_path)\n",
    "# save_df_to_es_format(df, es_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_to_query(title: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a title to a query.\n",
    "    Split the title by whitespace, and use up to 4 words as the query.\n",
    "    If the title has less than 4 words, use the whole title as the query.\n",
    "    \"\"\"\n",
    "    return \" \".join(title.split()[:4])\n",
    "\n",
    "\n",
    "def generate_vespa_query_file(df, num_queries: int, seed=int):\n",
    "    \"\"\"\n",
    "    Generate a query file for Vespa.\n",
    "    Sample format:\n",
    "    ```json\n",
    "    /search/\n",
    "    {\"yql\": \"select * from sources * where userQuery()\", \"ranking.profile\": \"bm25\", \"query\": \"small money clip leather wallet\"}\n",
    "    /search/\n",
    "    {\"yql\": \"select * from sources * where userQuery()\", \"ranking.profile\": \"bm25\", \"query\": \"blue nike shoes\"}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    df_sample = df.sample(num_queries, random_state=seed)\n",
    "    df_sample[\"query\"] = df_sample[\"title\"].apply(title_to_query)\n",
    "    query_file = Path(OUTPUT_DIR) / f\"vespa_query_sample-{num_queries}.jsonl\"\n",
    "    search_string = \"/search/\\n\"\n",
    "    with open(query_file, \"w\") as f:\n",
    "        for index, row in df_sample.iterrows():\n",
    "            f.write(search_string)\n",
    "            query = {\n",
    "                \"yql\": \"select * from sources * where userQuery();\",\n",
    "                \"ranking.profile\": \"default\",\n",
    "                \"query\": row[\"query\"],\n",
    "            }\n",
    "            f.write(json.dumps(query) + \"\\n\")\n",
    "    return query_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weakAnd + bm25 ranking:\n",
    "\n",
    "vespa query \"yql=select \\* from product where userQuery()\" ranking.profile=bm25 \"query=Small MONEY CLIP Leather Wallet\"\n",
    "\n",
    "### nearestNeighbor + closeness ranking:\n",
    "\n",
    "vespa query \"yql=select \\* from product where ({targetHits:100}nearestNeighbor(embedding,q_embedding))\" ranking.profile=closeness \"input.query(q_embedding)=[]\"\n",
    "\n",
    "Note: Use embedding from id:product:product::4, title:\"Small MONEY CLIP Leather Wallet ID Bag Cash Holder Credit Card Cover Case Pouch\"\n",
    "\n",
    "### Hybrid:\n",
    "\n",
    "vespa query \"yql=select \\* from product where ({targetHits:10}nearestNeighbor(embedding,q_embedding)) or userQuery()\" ranking.profile=hybrid \"query=Small MONEY CLIP Leather Wallet\" \"input.query(q_embedding)=[]\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
